{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cassandra-driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Data for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ace-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for existing CSVs\n",
    "#base_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/\"  # Update to your directory\n",
    "import pandas as pd\n",
    "\n",
    "# Load the individual CSV files\n",
    "instructions_df = pd.read_csv(\"/Users/bhavikpatel/Desktop/RAG/RAG_Data/instructions.csv\")\n",
    "html_components_df = pd.read_csv(\"/Users/bhavikpatel/Desktop/RAG/RAG_Data/html_components.csv\")\n",
    "css_styles_df = pd.read_csv(\"/Users/bhavikpatel/Desktop/RAG/RAG_Data/css_styles.csv\")\n",
    "\n",
    "# Create a list to hold the unified rows\n",
    "unified_data = []\n",
    "\n",
    "# Add Instructions to the unified data\n",
    "for _, row in instructions_df.iterrows():\n",
    "    unified_data.append({\n",
    "        \"ID\": f\"Instruction-{row['instruction_id']}\",\n",
    "        \"Source_Type\": \"Instruction\",\n",
    "        \"Source_ID\": row[\"instruction_id\"],\n",
    "        \"Text_Representation\": row[\"text\"]\n",
    "    })\n",
    "\n",
    "# Add HTML Components to the unified data\n",
    "for _, row in html_components_df.iterrows():\n",
    "    text_representation = f\"Name: {row['name']}, Attributes: {row['attributes']}, Context: {row['component_context']}.\"\n",
    "    unified_data.append({\n",
    "        \"ID\": f\"HTML-{row['component_id']}\",\n",
    "        \"Source_Type\": \"HTML_Component\",\n",
    "        \"Source_ID\": row[\"component_id\"],\n",
    "        \"Text_Representation\": text_representation\n",
    "    })\n",
    "\n",
    "# Add CSS Styles to the unified data\n",
    "for _, row in css_styles_df.iterrows():\n",
    "    text_representation = (f\"Selector: {row['selector']}, Properties: {row['properties']}, \"\n",
    "                           f\"Context: {row['description']}.\")\n",
    "    unified_data.append({\n",
    "        \"ID\": f\"CSS-{row['style_id']}\",\n",
    "        \"Source_Type\": \"CSS_Style\",\n",
    "        \"Source_ID\": row[\"style_id\"],\n",
    "        \"Text_Representation\": text_representation\n",
    "    })\n",
    "\n",
    "# Convert the unified data to a DataFrame\n",
    "unified_df = pd.DataFrame(unified_data)\n",
    "\n",
    "# Save the unified DataFrame to a CSV file\n",
    "output_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data.csv\"\n",
    "unified_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the content\n",
    "print(\"Unified Data for Embedding:\")\n",
    "print(unified_df.head())\n",
    "\n",
    "# Confirm the path of the saved file\n",
    "print(f\"Data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the unified CSV file\n",
    "file_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data.csv\"  # Update this path to the actual location of the file\n",
    "try:\n",
    "    unified_df = pd.read_csv(file_path)\n",
    "    print(\"Unified CSV loaded successfully. Here's a preview:\")\n",
    "    display(unified_df.head())\n",
    "    print(\"\\nColumn Names:\")\n",
    "    print(unified_df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the structured CSV file\n",
    "file_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display a snippet of the data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Verify model is loaded\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the 'Text_Representation' column\n",
    "data['Embedding'] = data['Text_Representation'].apply(lambda x: model.encode(str(x)).tolist())\n",
    "\n",
    "# Confirm embeddings are generated\n",
    "print(\"Embeddings generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with embeddings to a new CSV file\n",
    "output_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data_with_embeddings.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data with embeddings saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file with embeddings\n",
    "file_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data_with_embeddings.csv\"\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'Embedding' column from string to list\n",
    "data['Embedding'] = data['Embedding'].apply(lambda x: np.array(eval(x)))\n",
    "\n",
    "# Display a snippet of the data\n",
    "print(data.head())\n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a sample query\n",
    "query = \"ecommerce Generate a responsive navigation bar with call to action button\"\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding = model.encode(query).reshape(1, -1)\n",
    "\n",
    "# Confirm the query embedding\n",
    "print(\"Query embedding generated successfully!\")\n",
    "\n",
    "# Extract embeddings from the data\n",
    "embeddings = np.vstack(data['Embedding'])\n",
    "\n",
    "# Compute cosine similarities between the query and all embeddings\n",
    "similarities = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "\n",
    "# Add the similarity scores to the DataFrame\n",
    "data['Similarity'] = similarities\n",
    "\n",
    "# Display the top 5 similar records\n",
    "top_matches = data.sort_values(by='Similarity', ascending=False).head(5)\n",
    "\n",
    "print(\"Top Matches:\")\n",
    "print(top_matches[['ID', 'Text_Representation', 'Similarity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating connection and loading files in chunks for vectorization in AstraDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cassandra-driver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade astrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#add client below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the client\n",
    "client = DataAPIClient(\"\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "  \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "\n",
    "print(f\"Connected to Astra DB: {db.list_collection_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE previous Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the DataAPIClient\n",
    "client = DataAPIClient(\"your client\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "    \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "collection_name = \"ssai_vectordb\"\n",
    "\n",
    "# Retrieve the collection\n",
    "collection = db.get_collection(collection_name)\n",
    "\n",
    "# Function to delete all documents in the collection\n",
    "def delete_all_documents(collection):\n",
    "    try:\n",
    "        # Use delete_many with an empty filter to delete all documents\n",
    "        result = collection.delete_many({})\n",
    "        print(f\"All documents deleted successfully! Deleted count: {result.deleted_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting documents: {e}\")\n",
    "\n",
    "# Delete all documents\n",
    "delete_all_documents(collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Chunk wise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the DataAPIClient\n",
    "client = DataAPIClient(\"your client\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "    \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "\n",
    "# Define your collection name and file path\n",
    "collection_name = \"ssai_vectordb\"\n",
    "file_path = \"/Users/bhavikpatel/Desktop/RAG/RAG_Data/unified_data_with_embeddings.csv\"\n",
    "chunk_size = 500  # Reduce the number of rows per chunk to avoid server overload\n",
    "\n",
    "# Retrieve the collection object\n",
    "collection = db.get_collection(collection_name)\n",
    "\n",
    "# Function to upload a chunk to Astra DB with retry logic\n",
    "def upload_chunk_to_astra(chunk, collection):\n",
    "    documents = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        try:\n",
    "            # Prepare the document to upload\n",
    "            document = {\n",
    "                \"id\": row[\"ID\"],  # Primary key\n",
    "                \"source_type\": row[\"Source_Type\"],\n",
    "                \"source_id\": row[\"Source_ID\"],\n",
    "                \"text_representation\": row[\"Text_Representation\"],\n",
    "                \"embedding\": eval(row[\"Embedding\"])  # Ensure embeddings are properly formatted (e.g., lists)\n",
    "            }\n",
    "            documents.append(document)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row ID {row['ID']}: {e}\")\n",
    "\n",
    "    # Insert documents in bulk with retry logic\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if documents:\n",
    "                collection.insert_many(documents)\n",
    "                print(f\"Uploaded {len(documents)} documents successfully.\")\n",
    "            break  # Exit retry loop if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Error during upload attempt {attempt + 1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this chunk.\")\n",
    "\n",
    "# Read the CSV in chunks and upload\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "    print(f\"Uploading Chunk {i+1}...\")\n",
    "    upload_chunk_to_astra(chunk, collection)\n",
    "    print(f\"Chunk {i+1} uploaded successfully. Waiting before next chunk...\")\n",
    "    time.sleep(1)  # Add delay between chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the AstraDB data in VS Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install astrapy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the DataAPIClient\n",
    "client = DataAPIClient(\"your client\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "    \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "\n",
    "# Retrieve the collection\n",
    "collection_name = \"ssai_vectordb\"\n",
    "collection = db.get_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch documents in chunks\n",
    "def fetch_all_documents_in_chunks(collection, chunk_size=100):\n",
    "    documents = []\n",
    "    offset = 0\n",
    "    try:\n",
    "        while True:\n",
    "            chunk = list(collection.find({}).limit(chunk_size).skip(offset))\n",
    "            if not chunk:\n",
    "                break\n",
    "            documents.extend(chunk)\n",
    "            offset += chunk_size\n",
    "        print(f\"Retrieved {len(documents)} documents successfully!\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch documents\n",
    "documents = fetch_all_documents_in_chunks(collection)\n",
    "\n",
    "if documents:\n",
    "    print(\"Sample Document:\", documents[0])\n",
    "\n",
    "# Validate embeddings\n",
    "for doc in documents:\n",
    "    if not isinstance(doc.get(\"embedding\"), list):\n",
    "        print(f\"Invalid embedding format for document ID {doc['id']}\")\n",
    "# Display the first document (for testing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test The data by retriving based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Import DataAPIClient\n",
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the DataAPIClient\n",
    "client = DataAPIClient(\"your client\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "    \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "\n",
    "# Define your collection name\n",
    "collection_name = \"ssai_vectordb\"\n",
    "collection = db.get_collection(collection_name)\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use a lightweight SBERT model\n",
    "\n",
    "#Function to retrieve similar documents\n",
    "def retrieve_similar_documents(query_text, documents, top_k=3):\n",
    "    try:\n",
    "        # Embed the query\n",
    "        query_embedding = sbert_model.encode([query_text], clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "        # Extract embeddings and document IDs\n",
    "        embeddings = np.array([doc[\"embedding\"] for doc in documents])\n",
    "        ids = [doc[\"id\"] for doc in documents]\n",
    "        texts = [doc[\"text_representation\"] for doc in documents]\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity([query_embedding], embeddings).flatten()\n",
    "\n",
    "        # Get top K most similar documents\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        results = [{\"id\": ids[i], \"similarity\": similarities[i], \"text_representation\": texts[i]} for i in top_indices]\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Sample query text\n",
    "query_text = \"Design a modern header for a travel website with a minimalist layout.\"\n",
    "\n",
    "# Retrieve top 5 similar documents\n",
    "results = retrieve_similar_documents(query_text, documents, top_k=5)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top Similar Documents:\")\n",
    "for result in results:\n",
    "    print(f\"\\nID: {result['id']}\")\n",
    "    print(f\"Similarity: {result['similarity']:.2f}\")\n",
    "    print(f\"Text Representation: {result['text_representation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Initialize the DataAPIClient\n",
    "client = DataAPIClient(\"your client\")\n",
    "db = client.get_database_by_api_endpoint(\n",
    "    \"https://0b0963cb-6b3a-4145-86c2-be7762e6cd9c-westus3.apps.astra.datastax.com\"\n",
    ")\n",
    "\n",
    "# Define your collection name\n",
    "collection_name = \"ssai_vectordb\"\n",
    "collection = db.get_collection(collection_name)\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use a lightweight SBERT model\n",
    "\n",
    "# Function to fetch all documents from the collection\n",
    "def fetch_all_documents(collection):\n",
    "    try:\n",
    "        documents = list(collection.find({}))  # Retrieve all documents\n",
    "        print(f\"Retrieved {len(documents)} documents successfully!\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch all documents from the collection\n",
    "documents = fetch_all_documents(collection)\n",
    "\n",
    "# Function to fetch related HTML and CSS for an instruction\n",
    "def fetch_related_html_css(instruction_id, documents):\n",
    "    html_components = [doc for doc in documents if doc[\"source_type\"] == \"HTML_Component\" and doc[\"source_id\"] == instruction_id]\n",
    "    css_styles = [doc for doc in documents if doc[\"source_type\"] == \"CSS_Style\" and doc[\"source_id\"] == instruction_id]\n",
    "    return html_components, css_styles\n",
    "\n",
    "# Function to retrieve similar documents along with HTML and CSS\n",
    "def retrieve_similar_documents_with_code(query_text, documents, top_k=5):\n",
    "    try:\n",
    "        # Embed the query\n",
    "        query_embedding = sbert_model.encode([query_text], clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "        # Extract embeddings and document IDs\n",
    "        embeddings = np.array([eval(doc[\"embedding\"]) if isinstance(doc[\"embedding\"], str) else doc[\"embedding\"] for doc in documents])\n",
    "        ids = [doc[\"id\"] for doc in documents]\n",
    "        texts = [doc[\"text_representation\"] for doc in documents]\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity([query_embedding], embeddings).flatten()\n",
    "\n",
    "        # Get top K most similar documents\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        results = []\n",
    "        for i in top_indices:\n",
    "            instruction_id = ids[i]\n",
    "            html_components, css_styles = fetch_related_html_css(instruction_id, documents)\n",
    "            results.append({\n",
    "                \"id\": ids[i],\n",
    "                \"similarity\": similarities[i],\n",
    "                \"text_representation\": texts[i],\n",
    "                \"html_components\": html_components,\n",
    "                \"css_styles\": css_styles\n",
    "            })\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to create a heatmap for cosine similarity scores\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_similarity_heatmap(results):\n",
    "    try:\n",
    "        ids = [result[\"id\"] for result in results]\n",
    "        similarities = [result[\"similarity\"] for result in results]\n",
    "\n",
    "        # Create a heatmap\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap([similarities], annot=True, cmap=\"coolwarm\", xticklabels=ids, yticklabels=[\"Similarity\"])\n",
    "        plt.title(\"Cosine Similarity Heatmap for Retrieved Documents\")\n",
    "        plt.xlabel(\"Document IDs\")\n",
    "        plt.ylabel(\"Similarity\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating heatmap: {e}\")\n",
    "\n",
    "# Sample query text\n",
    "query_text = \"Design a modern header for a travel website with a minimalist layout.\"\n",
    "\n",
    "# Retrieve top 5 similar documents with associated HTML and CSS\n",
    "results = retrieve_similar_documents_with_code(query_text, documents, top_k=5)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top Similar Documents with Associated Code:\")\n",
    "for result in results:\n",
    "    print(f\"\\nID: {result['id']}\")\n",
    "    print(f\"Similarity: {result['similarity']:.2f}\")\n",
    "    print(f\"Text Representation: {result['text_representation']}\")\n",
    "    print(\"HTML Components:\")\n",
    "    for html in result[\"html_components\"]:\n",
    "        print(f\"  - {html['text_representation']}\")\n",
    "    print(\"CSS Styles:\")\n",
    "    for css in result[\"css_styles\"]:\n",
    "        print(f\"  - {css['text_representation']}\")\n",
    "\n",
    "# Create a heatmap for the similarity scores\n",
    "create_similarity_heatmap(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
